# InternVL-FastAPI-Runner

A lightweight, local deployment for multimodal visual-language inference using [OpenGVLab/InternVL3-1B](https://huggingface.co/OpenGVLab/InternVL3-1B) via [LMDeploy](https://github.com/InternLM/lmdeploy) and FastAPI. This project supports GPU-accelerated inference.

---

## ğŸ”§ Requirements

- GPU (recommended: NVIDIA RTX 4090 or higher)
- Docker & Docker Compose
- CUDA driver (recommended >= 12.1)
- Python 3.10+
- Linux or WSL (preferred)

---

## ğŸš€ Quickstart

### 1. Edit `.env`

```env
HOST=localhost
PORT=8000
UID=username
GPU_ID=0
````

### 2. Build and launch container

```bash
docker compose up --build
```

After launch, the FastAPI server will be accessible at `http://localhost:8000`.

---

## ğŸ”Œ API Endpoints

| Route    | Method | Description                             |
| -------- | ------ | --------------------------------------- |
| `/`      | GET    | Root check                              |
| `/ping`  | GET    | Health check                            |
| `/gpu`   | GET    | Check if CUDA/GPU is available          |
| `/infer` | POST   | Perform multimodal inference with image |

### Example: `/infer` Request Payload

```json
POST /infer
Content-Type: application/json

{
  "prompt": "Describe this image",
  "image_base64": "<base64 encoded image>"
}
```

---

## Testing

You can test the API using the included scripts in the `usecase/` folder:

### Basic endpoint tests:

```bash
python3 usecase/testAPI.py
```

### Image-based inference via remote URL:

```bash
python3 usecase/testLLM.py
```

---

## Project Structure

```
llm_test/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py              # FastAPI entrypoint
â”‚   â”œâ”€â”€ model_loader.py     # LMDeploy pipeline setup
â”‚   â”œâ”€â”€ utils.py            # Base64 image tools
â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ usecase/
â”‚   â”œâ”€â”€ testAPI.py          # Ping/GPU/Infer test
â”‚   â””â”€â”€ testLLM.py          # URL-based image inference
â”œâ”€â”€ .env                    # Local config (HOST, PORT, GPU ID)
â”œâ”€â”€ docker-compose.yml      # GPU container config
â”œâ”€â”€ Dockerfile              # PyTorch + LMDeploy base image
```

---

## âš™ï¸ LMDeploy Settings

`model_loader.py` configures the model with:

* `session_len=16384` â†’ maximum context length
* `tp=1` â†’ tensor parallelism (set to 1 GPU)
* `chat_template='internvl2_5'` â†’ InternVL official chat template

You can customize inference parameters:

```python
pipe(prompt, temperature=0.2, top_p=0.9, max_new_tokens=300)
```

---

## ğŸ“ References

* [OpenGVLab/InternVL3-1B on Hugging Face](https://huggingface.co/OpenGVLab/InternVL3-1B)
* [LMDeploy GitHub](https://github.com/InternLM/lmdeploy)
